# 基于OpenCV的多视图三维重建

## 摘要

本文研究了从运动中恢复结构SFM（Structure from Motion）方法以及三维重建的相关方法,基于开源代码实现了一个基于图像的三维模型重建流程。文中介绍了 SFM 的相关知识和算法。特征点提取和匹配分别使用AKAZE描述子和暴力匹配方法（Brute-force matcher ）。通过匹配的特征点对求得相机之间的对极几何关系，使用 RANSAC 8点法求得基础矩阵，然后对基础矩阵进行奇异值分解得到选择矩阵和平移向量。再使用直接线性方法（DLT）求得三维点坐标，（最后使用光束调整完成结果的统一优化，）得到稀疏点云。通过3D网格处理软件 MeshLab 进行表面重建可以反映出目标结构。重建的目标有两个，分别是帽子和艾思奇雕像。

## 1. 绪论

随着计算机技术的发展，虚拟现实和增强现实将会融入各个行业，如何把虚拟的场景逼真的呈现出来，或者如何将现实三维景物经济高效得转换到虚拟的数字世界中是当前研究的热门。

三维建模的数据来源大致可划分为三种：几何特征、距离和图像，三种数据来源对应着三种建模方法。早先最常用的是几何重建方法，根据对目标的几何信息的直接测量，进行结构重建。使用这种方法需要直接接触目标获取几何信息，并不能适用于所有目标。同时基于几何特征的重建需要操作专门的软件，不仅需要专业人员，而且工作量大且繁重。之后得到发展的是基于距离的重建方法，利用主动发射或者被动接受的探测仪器，能同时对平面信息和深度信息进行获取。目前应用广泛的三维扫描仪就是根据距离的远近生成相应的点，从而构建出目标的点云数据。这种方法获取的点云数据比较精确，但是扫描设备价格昂贵，不便于携带，功耗大，无法满足日常生活的需求。目前，相机、手机等消费级的拍摄设备非常普遍，如何使用图像来进行物体建模是一个值得思考的问题。因为相机拍照把三维世界的深度信息丢失了，恢复深度信息技术三维重建问题的关键，可以利用人工智能的技术对于单张视图直接估计深度，只是由于人工智能技术的泛化能力比较差，暂时还不能依靠单张视图来实现三维重建。传统上我们可以利用多张视图的相对关系去估计深度信息，不过需要的运算时间相对较长，图像越多，运算时间越长，物体重建精度就越好，所以我的研究方向是在尽量少的图像输入情况下实现较好的三维重建。

## 2. 多视图立体几何基础概念

多视图立体视觉(英文：Multiple View Stereo)作为计算机视觉中的一个重要方向，主要解决从若干幅多视图几何的二维图像中恢复三维信息的问题。摄像机拍得的照片是真实世界物体在成像表面的投影，二维的图像丢失了深度信息，所以恢复出深度信息就是三维重建的关键。

人的眼睛之所以能够分辨出三维的世界，就是因为两只眼睛从不同的角度观察同一个物体，从而两只眼各自看到的图像有一个视差，根据视差就得到了不同点的深度信息。

从图像到立体其实是坐标系的变换，从一个二维平面坐标系变换到一个三维坐标系，具体来说就是从相机坐标系变换到世界坐标系，所以需要了解不同坐标系之间的变换关系。为了得到视差，就需要从不同的角度观察物体，对于相机来说，就是从不同角度拍摄照片，为了描述相机的模型需要引入针孔相机模型，并且需要处理相机成像的畸变问题。从二维坐标变换到三维坐标需要对极约束的几何知识。

### 2.1 针孔相机模型

![针孔相机模型](C:\Users\MY\Documents\GitHub\cloudimg\image-20201115225420345.png)

图1. 针孔相机模型

来自场景或一个很远的物体的点发射过来的光线从小孔进入，各点被“投影”到成像表面。其结果是在图像平面（也称为投影平面）上，图像被聚焦，与远处物体相关的图像大小用一个相机参数来描述：焦距f。对于理想的针
孔相机，从针孔到屏幕的距离就是焦距。如图1 所示，f是相机焦距， Z是相机到物体的距离， X是物体的长度， x是图像平面上的物体图像。在该图中，我们可以通过相似三角形得到$-x/f = X/Z$ ， 或:
$$
-x = f\cdot \frac{X}{Z}
$$
在图2中，把图像平面和针孔平面交换位置，这种模型与上述模型等价，但其数学形式更简单，上述公式就变成了：
$$
\frac{x}{f} = \frac{X}{Z}
$$
![图像平面放到针孔前方](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201116070535280.png)

图2

在图2所示模型中，针孔中的点被理解为投影中心。每一条光线从远处物体的某个点出发，到达投影中心。光轴与图像平面的交点被称为"主点" (principal point) 。在针孔平面上 ，远处物体的图像与图1 中的图像大小完全一
致。光束与图像平面相交生成图像，而平面与投影中心的距离是f。这样便形成了比之前更容易理解的相似三角形关系$\frac{x}{f} = \frac{X}{Z}$。负号被去掉了，因为目标图像不再是倒立的。

因为相机里的成像装置通常不在光轴上，所以需要引入两个新的参数 $c_x$和$c_y$ ，以描述成像装置对投影屏幕坐标中心可能的偏移。

因为我们希望得到的是像素单位的坐标，才能对图像进行处理。焦距f 的单位是米（m），而x 的单位是像素（pixel），所以从米到像素需要一个比例因子s，另外，单个像素在低价成像装置上是矩形而不是正方形，x 和 y两个方向需要不同的比例因子$s_x$和$s_y$。所以我们引入两个焦距 $f_x 和 f_y$ ，
$$
f_x = f\cdot s_x \\
f_y = f\cdot s_y
$$
$f_x$和$f_y$ 的单位都是像素。 $s_x$和$s_y$以及物理焦距f 均不能在相机标定过程中直接测量，只有组合量$f_x$和$f_y$可以直接计算出来而不必拆除相机去直接测量其部件。

### 2.2 透镜畸变

真实的相机镜头并不是像在上文中介绍的一个针孔，而是采用一个凸透镜，以便获得更大的进光量。但是采用凸透镜会引入径向畸变，即在半径方向上，离光心越远得光线比靠近透镜中心的光线弯曲很多，图像畸变越严重，导致直线变得弯曲，如图3。为了用数学模型对畸变结果进行描述。在 r=0 附近进行泰勒级数展开：
$$
x_corrected = x \cdot (1+k_1 r^2+k_2r^4 + k_3r^6) \\
y_corrected = y \cdot (1+k_1 r^2 + k_2r^4+k_3r^6)
$$
![径向畸变](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201116195031128.png)

图3

第二大常见的畸变是切向畸变。这种畸变是由于制造上的缺陷使透镜不与成像平面平行而产生的，如图4所示。当透镜不完全与图像平面平行的时候则会产生切向畸变；在廉价的相机中，这种现象发生在成像装置被粘在相机背面的时候。切向畸变可以用两个额外的参数 $p_1$ 和 $p_2$ 来表示：
$$
x_corrected = x+[2p_1xy + p_2(r^2+2x^2)] \\
y_corrected = y+[p_1(r^2+2y^2)+2p_2xy]
$$


![切向畸变](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201116195333547.png)

图4

根据对径向畸变和切向畸变的数学描述，我们可以使用5个畸变系数构建一个畸变矩阵来描述透镜畸变，即：
$$
[k_1 \ k_2 \ p_1 \ p_2 \ k_3]^T
$$


### 2.3 齐次坐标

要把图像平面的二维坐标$(x_i,y_i)$转换到立体空间的三维坐标$(X_i,Y_i,Z_i)$ ,需要增加一维坐标，可以先给二维坐标添加一个 1， 使其变成三维坐标，即$(x_i, \ y_i,\ 1)$ 。由于深度信息位置，所以同一个三维点的齐次坐标可能对应多个二维点，互相只相差了一个比例因子 z。使用齐次坐标，就可以使用矩阵运算来表述从三维到二维的投影关系:
$$
\vec{q} = M \cdot \vec{Q} \\
$$

$$
\vec{q}= \begin{bmatrix}x \\ y\\ w \end{bmatrix}\quad
M=\begin {bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix} \quad
\vec{Q}=\begin{bmatrix}X \\ Y \\ W \end{bmatrix}\\
$$

这个方程描述的就是：真实世界的三维坐标点Q，乘上相机的内参矩阵，就得到了二维图像平面上的坐标 q。

### 2.3 旋转矩阵和平移向量

从不同角度拍摄的图像对应不同的相机位姿，也对应不同的相机坐标系，为了把物体点从不同的相机坐标系下恢复到世界坐标系下，我们必须知道各个坐标系之间的转换关系。两个坐标系之间的变化（刚体运动）由两个部分组成：原点间的平移和三个轴的旋转。

对于一个向量来说，在不同的坐标系下有不同的坐标，但是向量本身使不变的，在每一个坐标系下，向量都等于坐标系的基底乘上向量的坐标。设某坐标系（e1, e2, e3）发生了一次**旋转**，变成了（e1', e2', e3'）

对于某个固定的向量a（向量本身不随坐标系旋转），坐标关系:
$$
\left[ \begin{array}{ccc}
	e_1, \ e_2, \ e_3 
\end{array} \right]
\left[ \begin{array}{c} a_1 \\ a_2 \\ a_3 \end{array} \right] =
\left[ \begin{array}{ccc}
	e_1^\prime, \ e_2^\prime,\ e_3^\prime  
\end{array} \right]
\left[ \begin{array}{c} a_1^\prime  \\ a_2^\prime  \\ a_3^\prime  \end{array} \right]
$$
左乘 $\left[ \begin{array}{c} e^T_1\\ e^T_2\\ e^T_3 \end{array} \right]$ , 得：左边的系数就变成了单位矩阵
$$
\left[ \begin{array}{c} a_1 \\ a_2 \\ a_3 \end{array} \right] =
\left[ \begin{array}{ccc}
	e^T_1e_1^\prime & e^T_1e_2^\prime & e^T_1e_3^\prime \\
	e^T_2e_1^\prime & e^T_2e_2^\prime & e^T_2e_3^\prime \\
	e^T_3e_1^\prime & e^T_3e_2^\prime & e^T_3e_3^\prime
\end{array}\right] 
\left[ \begin{array}{c} a_1^\prime \\ a_2^\prime \\ a_3^\prime \end{array} \right] \triangleq R\vec{a^\prime}
$$
R 是由两组基之间的内积组成，刻画了前后同一个向量的坐标变换关系。只要旋转是一样的，这个矩阵就是一样的。可以说，矩阵R 描述了旋转本身。因此，称为旋转矩阵( Rotation Matrix )。同时，该矩阵各分量是两个坐标系基的内积，由于基向量的长度为1 ，所以实际上是各基向量夹角的余弦值。所以这个矩阵也叫方向余弦矩阵( Direction Cosin巳Matrix)。我们后文统一称它为旋转矩阵。

坐标系之间的平移就是原点间的平移，可用一个三元向量即可表示。

### 2.4 对极几何



## 3. SfM方法

SFM( structure from motion) 算法时一种基于多福无序图片进行三维重建的算法，顾名思义在相机的运动中（不同时间不同角度拍摄的图片）恢复物体的三维结构。

### 3.1 特征点提取和匹配

特征点是图像中，在旋转、平移、缩放以及对场景物体的其他更复杂的变换中不受影响的部分，可以是角点、边缘和色块。用描述子来量化特征点附近的情况，用以区分和辨别不同的特征点。

OpenCV提供了广泛的2D特征检测器和描述子。OpenCV API 的最新成员之一是AKAZE 特征提取器和检测器，它在计算速度和转换的鲁棒性之间提供了非常好的折中。AKAZE的表现优于其他突出特征，例如ORB（Oriented BRIEF）和 SURF（Speeded Up Robust Features）。AKAZE为每张图像计算AKAZE特征，并将它们分别保存在 KeyPoints 和 Descriptions 数组中。

通过比较描述子之间的距离，可以匹配两幅图像中相同的特征点，形成一个特征点对。AKAZE特征描述符是二进制字符串，意味着在匹配时它们不能被视为二进制编码的数值，它们必须使用逐位运算符进行位与位之间的比较。OpenCV为二进制特征匹配器提供了海明距离（Hamming distance）度量，其原理是计算两位序列之间不正确匹配的数量。

OpenCV 提供了两种Matching方式：暴力匹配(Brute-force-matcher) 和 快速最近邻搜索算法 (Flann-based-matcher) 。暴力方法找到点集1中每个descriptor 在点集2中距离最近的descriptor；找寻到的距离最小就认为匹配。在调用cv::BFMatcher 方法时，设置crossCheck 为 True，匹配条件就会更加严格，只有到A中的第i个特征点与B中的第j个特征点距离最近，并且B中的第j个特征点到A中的第i个特征点也是最近时才会返回最佳匹配(i,j)，即这两个特征点要互相匹配才行。第二种快速最近邻搜索算法寻找是一个对大数据集和高维特征进行最近邻搜索的算法的集合，在面对大数据集时它的效果要好于BFMatcher。

### 3.2 估计基本矩阵和本征矩阵

本征矩阵E：包含关于物理空间中两个摄像机的平移和旋转的信息。将左侧摄像机所观察到的点P的物理坐标位置与右侧摄像机所看到的相同的点位置相关联。因为基本矩阵F 包含了本征矩阵E，而F可以由特征点对估计出来，所以先求基本矩阵F，再根据F求E。

基本矩阵F：包含本征矩阵E 和两个摄像机的内参矩阵。由于F包含内参信息，它可以在像素坐标系上将两台摄像机关联。将图像坐标系中一个摄像机的像平面上的点与另一个摄像机的像平面上的点关联。

使用RANSAC 算法估计基本矩阵F：通过多次迭代，选取能被很好描述的点最多的对应的F，每次迭代从特征点对中选取8个点对，用直接线性方法（DLT）计算F，记录内点（适应数学模型的数据）的数目。

旋转矩阵和平移向量可由本征矩阵E经过SVD分解得到。

### 3.3 摄像机标定

在由基本矩阵F 计算本征矩阵的过程中，需要使用相机内参矩阵。如果有条件可以事先测出摄像机内参矩阵，会使计算结果更精确。

#### 3.3.1 提取角点信息

用cv::findChessboardCorners()找到棋盘角点。

`ret,corners = cv2.findChessboardCorners(gray,(8,6),None) `

传入拍摄的棋盘图像，必须是8位的灰度或者彩色图像，指定每个棋盘上内焦点的行列数（8, 6），一般情况下，行列数不要相同，便于后续标定程序识别标定板的方向。该函数可以输出存储角点位置的矩阵corners。

### 2.2 提取亚像素角点信息

为了提高标定精度，需要在初步提取的角点信息上进一步提取亚像素信息，降低相机标定偏差，常用的方法是cornerSubPix。

`corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria`

第一个参数输入8位灰度图像，第二个参数是初始的角点坐标向量，同时作为亚像素坐标位置的输出，所以需要是浮点型数据；第三个参数region_size，角点搜索窗口的尺寸；第四个参数zeroZone，死区为不对搜索区的中央位置做求和运算的区域。它是用来避免自相关矩阵出现某些可能的奇异性。当值为（-1，-1）时表示没有死区；第五个参数criteria，定义求角点的迭代过程的终止条件，可以为迭代次数和角点精度两者的组合。

然后可以使用drawChessboardCorners函数绘制被成功标定的角点

### 2.3 求相机内参矩阵

获取到棋盘标定图的内角点图像坐标之后，就可以使用calibrateCamera函数进行标定，计算相机内参和外参系数。

`ret,mrx,dist,rvecs,tvecs = cv2.calibrateCamera(objpoints,imgpoints,(8,6),None,None)`

* objectPoints 是向量的向量，每个向量包含特定图像的标定图案上的点的坐标。那些坐标在物体坐标系中，所以可以简单使它们在x维和y维中是整数，而在z 维中为零。
* imagePoints 它也是向量的向量，并且包含每个图像中找到的每个点的位置。如果使用的是棋盘，每个向量将是相应图像中的角点输出矩阵。
* imageSize 只是告诉cv::calibrateCamera()提取imagePoints的点的图像有多大（以像素为单位）

输出 mrx 包含线性内在参数 3x3矩阵，dist 畸变系数向量，rvecs 向量的向量，像输入点的矩阵，包含每个棋盘的旋转矩阵，tvecs 平移矩阵

### 2.4 利用标定结果对棋盘图进行矫正

使用undistort 进行矫正：

`dst = cv2.undistort(img,mrx,dist,None,newcameramtx)`

第一个参数img，输入参数，代表畸变的原始图像； 
第二个参数dst，矫正后的输出图像，跟输入图像具有相同的类型和大小；

第三个参数cameraMatrix为之前求得的相机的内参矩阵；

第四个参数distCoeffs为之前求得的相机畸变矩阵；

第五个参数newCameraMatrix，默认跟cameraMatrix保持一致；

### 3.3  求三维点坐标

直接线性方法是根据一个三维空间点的坐标同时满足在两个二维视图的投影方程，两个投影方程可转换维齐次线性方程组解决。投影矩阵P是由内参矩阵K，旋转矩阵R和平移向量t 组成，将各个参数带入便可求得特征点的三维空间坐标。

调用pts2ply 函数生成.ply 点云文件。

### 3.4 光束调整

### 3.5 点云的显示

使用开源软件meshlab可以查看并用稠密点云代替稀疏点云。

![image-20201102164201171](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201102164201171.png)

移除未引用点：

![image-20201102164137420](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201102164137420.png)

网格化：Filter –> Remeshing, Simplification and Reconstruction–> Surface Reconstruction: Poisson.

利用Poisson Surface Reconstruction算法由稠密點雲生成多邊形網格表面。參數可調， Octree Depth：控制着網格的細節，此值越大細節越豐富但佔內存越大運行起來慢，一般設10，可慢慢調大。



## 4. 重建结果与分析

### 4.1 摄像机标定

角点检测：

![image-20201104095421367](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201104095421367.png)

摄像机矩阵：

```
[[681.12423557   0.         303.78759146]
 [  0.         680.58815505 223.50308226]
 [  0.           0.           1.        ]]
```

矫正结果：

![calibresult](C:\Users\MY\Documents\GitHub\Exercise-Python\OpenCV实验\calibresult.png)

### 4.2 特征点提取与匹配

特征点检测：

![image-20201104101355758](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201104101355758.png)

特征点匹配：

![image-20201104104232009](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201104104232009.png)

稀疏点云：

![image-20201104105737409](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201104105737409.png)



## 5. 总结和展望

本课题实验中只采用了两张不同角度的图像，生成的点云几乎没有立体的分布，在之后的工作中应该采用多张不同的角度图像进行估计。目前采用的算法需要相机的参数矩阵，如果参数矩阵存在较大偏差，则估计结果会非常不准确，后面的工作会加入光束调整和迭代优化，不断精确相机矩阵，实现更好的效果。

## 参考文献

《Mastering OpenCV 4 - third edition》唐灿 译 ISBN：9787111645771 第二章

 [1]鲁晨曦. 基于图像的全局SFM三维模型重建方法研究与实现[D].电子科技大学,2017.