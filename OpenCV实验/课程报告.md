# 基于OpenCV的多视图三维重建

## 摘要

本文研究了从运动中恢复结构SFM（Structure from Motion）方法以及三维重建的相关方法,基于开源代码实现了一个基于图像的三维模型重建流程。文中介绍了 SFM 的相关知识和算法。特征点提取和匹配分别使用AKAZE描述子和暴力匹配方法（Brute-force matcher ）。通过匹配的特征点对求得相机之间的对极几何关系，使用 RANSAC 8点法求得基础矩阵，然后对基础矩阵进行奇异值分解得到选择矩阵和平移向量。再使用直接线性方法（DLT）求得三维点坐标，（最后使用光束调整完成结果的统一优化，）得到稀疏点云。通过3D网格处理软件 MeshLab 进行表面重建可以反映出目标结构。重建的目标有两个，分别是帽子和艾思奇雕像。

## 1. 绪论

随着计算机技术的发展，虚拟现实和增强现实将会融入各个行业，如何把虚拟的场景逼真的呈现出来，或者如何将现实三维景物经济高效得转换到虚拟的数字世界中是当前研究的热门。

三维建模的数据来源大致可划分为三种：几何特征、距离和图像，三种数据来源对应着三种建模方法。早先最常用的是几何重建方法，根据对目标的几何信息的直接测量，进行结构重建。使用这种方法需要直接接触目标获取几何信息，并不能适用于所有目标。同时基于几何特征的重建需要操作专门的软件，不仅需要专业人员，而且工作量大且繁重。之后得到发展的是基于距离的重建方法，利用主动发射或者被动接受的探测仪器，能同时对平面信息和深度信息进行获取。目前应用广泛的三维扫描仪就是根据距离的远近生成相应的点，从而构建出目标的点云数据。这种方法获取的点云数据比较精确，但是扫描设备价格昂贵，不便于携带，功耗大，无法满足日常生活的需求。目前，相机、手机等消费级的拍摄设备非常普遍，如何使用图像来进行物体建模是一个值得思考的问题。因为相机拍照把三维世界的深度信息丢失了，恢复深度信息技术三维重建问题的关键，可以利用人工智能的技术对于单张视图直接估计深度，只是由于人工智能技术的泛化能力比较差，暂时还不能依靠单张视图来实现三维重建。传统上我们可以利用多张视图的相对关系去估计深度信息，不过需要的运算时间相对较长，图像越多，运算时间越长，物体重建精度就越好，所以我的研究方向是在尽量少的图像输入情况下实现较好的三维重建。

## 2. 多视图立体几何基础概念

多视图立体视觉(英文：Multiple View Stereo)作为计算机视觉中的一个重要方向，主要解决从若干幅多视图几何的二维图像中恢复三维信息的问题。





## 3. SfM方法

SFM( structure from motion) 算法时一种基于多福无序图片进行三维重建的算法，顾名思义在相机的运动中（不同时间不同角度拍摄的图片）恢复物体的三维结构。

### 3.1 特征点提取和匹配

特征点是图像中，在旋转、平移、缩放以及对场景物体的其他更复杂的变换中不受影响的部分，可以是角点、边缘和色块。用描述子来量化特征点附近的情况，用以区分和辨别不同的特征点。

OpenCV提供了广泛的2D特征检测器和描述子。OpenCV API 的最新成员之一是AKAZE 特征提取器和检测器，它在计算速度和转换的鲁棒性之间提供了非常好的折中。AKAZE的表现优于其他突出特征，例如ORB（Oriented BRIEF）和 SURF（Speeded Up Robust Features）。AKAZE为每张图像计算AKAZE特征，并将它们分别保存在 KeyPoints 和 Descriptions 数组中。

通过比较描述子之间的距离，可以匹配两幅图像中相同的特征点，形成一个特征点对。AKAZE特征描述符是二进制字符串，意味着在匹配时它们不能被视为二进制编码的数值，它们必须使用逐位运算符进行位与位之间的比较。OpenCV为二进制特征匹配器提供了海明距离（Hamming distance）度量，其原理是计算两位序列之间不正确匹配的数量。

OpenCV 提供了两种Matching方式：暴力匹配(Brute-force-matcher) 和 快速最近邻搜索算法 (Flann-based-matcher) 。暴力方法找到点集1中每个descriptor 在点集2中距离最近的descriptor；找寻到的距离最小就认为匹配。在调用cv::BFMatcher 方法时，设置crossCheck 为 True，匹配条件就会更加严格，只有到A中的第i个特征点与B中的第j个特征点距离最近，并且B中的第j个特征点到A中的第i个特征点也是最近时才会返回最佳匹配(i,j)，即这两个特征点要互相匹配才行。第二种快速最近邻搜索算法寻找是一个对大数据集和高维特征进行最近邻搜索的算法的集合，在面对大数据集时它的效果要好于BFMatcher。

### 3.2 估计基本矩阵和本征矩阵

本征矩阵E：包含关于物理空间中两个摄像机的平移和旋转的信息。将左侧摄像机所观察到的点P的物理坐标位置与右侧摄像机所看到的相同的点位置相关联。因为基本矩阵F 包含了本征矩阵E，而F可以由特征点对估计出来，所以先求基本矩阵F，再根据F求E。

基本矩阵F：包含本征矩阵E 和两个摄像机的内参矩阵。由于F包含内参信息，它可以在像素坐标系上将两台摄像机关联。将图像坐标系中一个摄像机的像平面上的点与另一个摄像机的像平面上的点关联。

使用RANSAC 算法估计基本矩阵F：通过多次迭代，选取能被很好描述的点最多的对应的F，每次迭代从特征点对中选取8个点对，用直接线性方法（DLT）计算F，记录内点（适应数学模型的数据）的数目。

旋转矩阵和平移向量可由本征矩阵E经过SVD分解得到。

### 3.3 摄像机标定

在由基本矩阵F 计算本征矩阵的过程中，需要使用相机内参矩阵。如果有条件可以事先测出摄像机内参矩阵，会使计算结果更精确。

#### 3.3.1 提取角点信息

用cv::findChessboardCorners()找到棋盘角点。

`ret,corners = cv2.findChessboardCorners(gray,(8,6),None) `

传入拍摄的棋盘图像，必须是8位的灰度或者彩色图像，指定每个棋盘上内焦点的行列数（8, 6），一般情况下，行列数不要相同，便于后续标定程序识别标定板的方向。该函数可以输出存储角点位置的矩阵corners。

### 2.2 提取亚像素角点信息

为了提高标定精度，需要在初步提取的角点信息上进一步提取亚像素信息，降低相机标定偏差，常用的方法是cornerSubPix。

`corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria`

第一个参数输入8位灰度图像，第二个参数是初始的角点坐标向量，同时作为亚像素坐标位置的输出，所以需要是浮点型数据；第三个参数region_size，角点搜索窗口的尺寸；第四个参数zeroZone，死区为不对搜索区的中央位置做求和运算的区域。它是用来避免自相关矩阵出现某些可能的奇异性。当值为（-1，-1）时表示没有死区；第五个参数criteria，定义求角点的迭代过程的终止条件，可以为迭代次数和角点精度两者的组合。

然后可以使用drawChessboardCorners函数绘制被成功标定的角点

### 2.3 求相机内参矩阵

获取到棋盘标定图的内角点图像坐标之后，就可以使用calibrateCamera函数进行标定，计算相机内参和外参系数。

`ret,mrx,dist,rvecs,tvecs = cv2.calibrateCamera(objpoints,imgpoints,(8,6),None,None)`

* objectPoints 是向量的向量，每个向量包含特定图像的标定图案上的点的坐标。那些坐标在物体坐标系中，所以可以简单使它们在x维和y维中是整数，而在z 维中为零。
* imagePoints 它也是向量的向量，并且包含每个图像中找到的每个点的位置。如果使用的是棋盘，每个向量将是相应图像中的角点输出矩阵。
* imageSize 只是告诉cv::calibrateCamera()提取imagePoints的点的图像有多大（以像素为单位）

输出 mrx 包含线性内在参数 3x3矩阵，dist 畸变系数向量，rvecs 向量的向量，像输入点的矩阵，包含每个棋盘的旋转矩阵，tvecs 平移矩阵

### 2.4 利用标定结果对棋盘图进行矫正

使用undistort 进行矫正：

`dst = cv2.undistort(img,mrx,dist,None,newcameramtx)`

第一个参数img，输入参数，代表畸变的原始图像； 
第二个参数dst，矫正后的输出图像，跟输入图像具有相同的类型和大小；

第三个参数cameraMatrix为之前求得的相机的内参矩阵；

第四个参数distCoeffs为之前求得的相机畸变矩阵；

第五个参数newCameraMatrix，默认跟cameraMatrix保持一致；

### 3.3  求三维点坐标

直接线性方法是根据一个三维空间点的坐标同时满足在两个二维视图的投影方程，两个投影方程可转换维齐次线性方程组解决。投影矩阵P是由内参矩阵K，旋转矩阵R和平移向量t 组成，将各个参数带入便可求得特征点的三维空间坐标。

调用pts2ply 函数生成.ply 点云文件。

### 3.4 光束调整

### 3.5 点云的显示

使用开源软件meshlab可以查看并用稠密点云代替稀疏点云。

![image-20201102164201171](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201102164201171.png)

移除未引用点：

![image-20201102164137420](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201102164137420.png)

网格化：Filter –> Remeshing, Simplification and Reconstruction–> Surface Reconstruction: Poisson.

利用Poisson Surface Reconstruction算法由稠密點雲生成多邊形網格表面。參數可調， Octree Depth：控制着網格的細節，此值越大細節越豐富但佔內存越大運行起來慢，一般設10，可慢慢調大。



## 4. 重建结果与分析

### 4.1 摄像机标定

角点检测：

![image-20201104095421367](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201104095421367.png)

摄像机矩阵：

```
[[681.12423557   0.         303.78759146]
 [  0.         680.58815505 223.50308226]
 [  0.           0.           1.        ]]
```

矫正结果：

![calibresult](C:\Users\MY\Documents\GitHub\Exercise-Python\OpenCV实验\calibresult.png)

### 4.2 特征点提取与匹配

特征点检测：

![image-20201104101355758](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201104101355758.png)

特征点匹配：

![image-20201104104232009](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201104104232009.png)

稀疏点云：

![image-20201104105737409](C:\Users\MY\AppData\Roaming\Typora\typora-user-images\image-20201104105737409.png)



## 5. 总结和展望

本课题实验中只采用了两张不同角度的图像，生成的点云几乎没有立体的分布，在之后的工作中应该采用多张不同的角度图像进行估计。目前采用的算法需要相机的参数矩阵，如果参数矩阵存在较大偏差，则估计结果会非常不准确，后面的工作会加入光束调整和迭代优化，不断精确相机矩阵，实现更好的效果。

## 参考文献

《Mastering OpenCV 4 - third edition》唐灿 译 ISBN：9787111645771 第二章

 [1]鲁晨曦. 基于图像的全局SFM三维模型重建方法研究与实现[D].电子科技大学,2017.