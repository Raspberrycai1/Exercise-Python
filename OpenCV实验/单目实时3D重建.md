2020/10/27

Content:

[toc]



# 一、Realtime Dynamic 3D Facial Reconstruction for Monocular  

[Video In-the-Wild ICCV 2017 PEOPLECAP](https://www.youtube.com/watch?v=EAdUCj1EkPY)

impressive!!!

# 二、Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone

[Video](https://www.youtube.com/watch?v=W95Vs4REUGM)

Abstract：

**online** mesh generation

# 三、基于移动设备的实时对象三维重建

浙江大学 姜慧文

移动设备引入苹果ARKit，获取相机图像及相机位姿

**并行分层投影方法**，提高了三维重建过程中模型投影的效率

输入RGB相机拍摄的序列帧图像，先用**稠密的图像矫正**和**块匹配算法**计算视差估计**深度图**，最后使用并行的**模型更新**算法将相机图像及深度图统合到一个**TSDF体表面模型**上，并利用并行的模型投影算法对用户进行反馈。

并行：OpenGL ES3 着色器并行计算，提高运行速度。

局限：重建后的模型尺寸是固定的

### 3.4.1 TSDF 模型

TSDF模型：模型中每个体素都表示成累积加权的有符号距离函数F(x)，F(x)表示位于坐标位置为x的体素距离最近的表面的距离。当用已知相机位姿的深度图对模型进行融合时，模型会在深度图中的深度表面对应位置形成一个表面；而当多次用深度图对模型进行融合时，会形成多个表面，每次融合深度图像时进行简单的加权，最后从体素网格中提取等值面以成为模型的表面。

在表面上的体素F(x)值为0，这些F(x) = 0 的点被称为过零点，通过寻找过零点的体素即可得到模型的表面。

图3.4（b) 中进行了两次融合，对于标注红色的体素p，第一次融合时，得到的表面靠右，p处于表面的Near方向，F(p)=-n，第二次融合时，得到表面靠左侧，p处于表面的Far方向，F(p) = n。最终的F(p) 平均两次融合的结果：F(p) = 0,也就是两次融合后 p 点成了过零点。

每次融合表面时，只处理Near 和 Far 两个方向到表面距离小于等于 $\mu$ 的体素，也就是将有符号距离函数截断到 $\mu$, 最终 F(x) 的范围属于[-$\mu$,$\mu$]。

上述描述可见，TSDF模型的缺点是要预设模型而限制了场景的尺度。

### 3.4.2 具体实现

本课题设定最终重建的模型是一个 包含 K x K x K 个体素的立方体。其中每个体素 p $\in$ P 有三个属性：

1. **F(p)** 表示此体素到最近的表面的有符号距离，也就是截断有符号距离函数的值
2. **C(p)** 表示此体素的RGB三通道颜色。
3. **W(p)** 表示此体素被更新过的次数，用以计算平均，也可以表示体素在此算法中的置信度。

在本文中将K 设为256

### 3.4.3 用纹理图表示TSDF 模型

纹理图一般是OpenGL 中给图元绘制纹理时传入的图片，有1\~4 个通道，每个通道保存一定的颜色值。使用时首先将CPU中的图像数据传入GPU的buffer中，然后将buffer的id 用uniform 传给着色器中，在着色器中得以按图像坐标访问纹理图的像素值。值得一提的是，着色器中访问纹理图的坐标是float格式的，所以对于非整像素格的部分，会自动地对周围的四个像素格的值进行双线性插值，得到平均的纹理值。

## 4.1 深度估计模块框架

通过**彩色图像**以及**相机位姿**，得到深度图像。这个深度图的位姿并不会和输入图像的位姿相同，而是从同一位置、另一姿态重新投影了彩色图像并计算了深度图像。

深度估计模块包含四个算法：

1. 图像矫正
2. 关键帧评分
3. 视差估计
4. 生成深度图

深度估计模块的**输入**为当前帧图像$I_i$，以及相机位姿估计模块得到的当前帧相机位姿$T_i$。关键帧选取部分只利用相机位姿对关键帧池中的历史帧进行评分，并选取分值最高的历史帧（$I_k$, $T_k$）与当前帧（$I_i$, $T_i$）组成关键帧对，一起进行**立体匹配**部分的计算。这其中历史帧（I_k, T_k）也被称为参考帧。

立体匹配部分主要就是对两张已知位姿的图像的每个像素进行匹配，得到每一像素的视差，以计算得到其深度。图像矫正部分输入需要处理的关键帧对$(I_j,T_j)$和$(I_k,T_k)$，使得两帧图像间的视差只存在于一个轴的方向。对矫正后的关键帧对：$(I_j',T_j')$和$(I_k',T_k')$ 进行匹配以得到两张视差图，再对两张视差图进行**左右一致性检测**和**三角测量**即可得到深度图。

### 图像矫正

给定两张有立体位姿的图像，“图像矫正” 意味着计算出对应每个图像视角的**变换矩阵**，使得两张原图像经过这个变换后可以保证**视角**的一个轴（通常是x轴）方向相同。两个相机光心的连线是基线，基线落在两个相机的焦平面上时，两个极点E1，E2都在无穷远，像平面也与基线平行。这时两个图像保持平行，两个极线也保持平行，**两图间的视差只存在于横向**，这就是图像矫正的结果。图像矫正使得寻找另一张图中的某一像素只需要横向查找即可，这就避免了普通情况时寻找极线的难度。

#### 计算变换矩阵

图像矫正并不改变光心C1，C2的位置，而是通过旋转变换改变相机的视角。所以可以定义两个矫正后的相机位姿：
$$
P_{n1} = [Q_{n1} | q_{n1}] = A[R|c_1];\\
P_{n2} = [Q_{n2} | q_{n2}] = A[R|c_2];
$$
其中p 是体素 R 是旋转矩阵，可以分解为三个单位向量：
$$
R = [r_1 \ r_2 \ r_3]
$$
这三个单位向量对应着矫正后相机姿态的三个轴向量。可以如下定义这三个向量：

1. 新的X轴平行于基线：$r_1 = (c_1 - c_2)/\left\| c_1 - c_2 \right \|$
2. 新的Y轴要与X轴正交，可以定义为：$r_2 = k \wedge r_1$，其中 k 可以是任意单位向量，这里将 k 定义为原来的 Z 轴方向。
3. 新的Z轴要与X、Y轴正交：$r_3 = r_1 \wedge r_2 $

#### 应用变换矩阵 

是把原图像根据变换矩阵投影到矫正后的图像。对于新图像中的一个像素点坐标$m_{n1}$，假设其对应的空间中一点的三维坐标为 w= (x,y,z)，则需要寻找 w 在原图像中的投影坐标 $m_{o1}$，也就是说需要寻找 $m_{o1}$ 到 $m_{n1}$ 之间的关系式。

首先定义矫正前的两个相机位姿为：
$$
P_{o1} = A[R_{o1}|c_1] = [Q_{o1} | q_{o1}];\\
P_{n1} = A[R_{n1}|c_2] = [Q_{n1} | q_{n1}];
$$
用齐次坐标表示投影关系：
$$
d\widetilde{m}_{o1} = K(R_{o1}^{-1}w - R_{o1}^{-1}c_1)\\
d\widetilde{m}_{n1} = K(R_{n1}^{-1}w - R_{n1}^{-1}c_1)
$$
推出：
$$
w = d_1Q_{o1}\widetilde{m}_{o1} + c_1 \\
w = d_2Q_{n1}\widetilde{m}_{n1} + c_1
$$
得：
$$
\widetilde{m}_{n1} = Q_{n1}^{-1}Q_{o1}\widetilde{m}_{o1}
$$
由此，可以定义矫正图像1 得变换矩阵为 3 x 3 的矩阵：$T_1 = Q_{n1}^{-1}Q_{o1} $ ；同理，矫正图像2 矩阵为：$T_2 = Q_{n2}^{-1}Q_{o2}$

## 4.4 关键帧评分算法

从历史帧中选择一帧与当前帧基线最合适的帧，组成关键帧对。如果两帧间基线差距太大，两帧重叠内容少，而且近处景物的视差太大而无法被估计到；如果两帧间基线差距太小，导致远处景物的视差太小（小于一个像素）而无法准确估计。

本文中使用两个评分项来评价关键帧对：相交评分项和视差评分项，最终的总评分为两项之和，而分值最高的一帧即被选中。

那我用双目是不是直接就有关键帧对了？

### 相交评分项

用来评价两个矫正后的图像之间的重叠程度，记为：$S_{\cap}(T_k)$。 由于矫正后的两个图像X 轴 方向是相同的，所以只要根据基线长度计算其重叠程度即可。最朴素的做法就是直接计算baseline的欧氏距离。

### 视差评分项

是未来控制拍摄到的三维点在两帧间的视差不要过大。可以将重建中的模型立方体的每个顶点分别投影到两帧的相机位姿中，如果都不超过设定的最大视差$d_{max}$ ， 则属于理想范围。实现中对模型立方体的每个顶点求一次在两图中的视差，得到最大的一个 $\delta_{max}(T_k) $，最终设视差评分项为：
$$
S_d(T_k) = -\left|1- \frac{\delta_{max}(T_k)}{d_{max}}\right|
$$
本文中设 $d_{max} = 64 $

## 4.5 视差估计算法

视差估计算法是将一张图像中的一个像素M1 ，找出在另一张图像中对应同一空间点 w 的像素位置 M2，以此得到两图中同一空间点的投影视差$\left\| M_1 - M_2 \right\| $，最终生成两张图对应的视差图。与此相对也存在这对多长视图角的视差估计方法，本文中为了更高的效率，只使用两张图构成图像对来估计视差图。视差图用每个像素的亮度值来表示此像素的视差值，像素点的亮度越亮，表示视差越大，也就是深度越小；反之，像素点的亮度越暗，则视差越小，深度越大。

在普通图像对之间做视差是一个很复杂的问题，但是在矫正后的图像对之间
做视差估计就相对容易，假设M1=(x1，y1)，M2=(x2，y2)，由前文中提到的图像矫正的性质可知 y1=y2，所以M1 点的视差就变成$\left\| x_1 -x_2 \right\|$，如图4.3所示。

![image-20201103104034449](C:\Users\MY\Documents\GitHub\cloudimg\图像矫正后视差.png)

向上的箭头分别表示两个相机的Z轴方向，橙色线表示两个相机的像平面，f 为焦距。空间点 w = (x,y,z) 的深度即为 z，视差 $\delta = \left\| x_1 - x_2 \right\|$

![image-20201103105910638](C:\Users\MY\Documents\GitHub\cloudimg\立体匹配示意图.png)

如图4.4所示为在矫正后的图像对之间做像素匹配的过程，左图中一像素点m(x，y)，其对应的右图中一像素点应位于(x，y) (x+d_max，y)之间，选取这范围内与m点最相似的点即为所求。但是实际上受光照等影响，这种做法的误差是非常大的，一般是选取m点为中心的一块 r×r 的区域做匹配，这种方法称为块匹配算法(英文：Block．match)。

### 4.5.1 块匹配算法

如上文所述，由于只用一个像素做视差匹配误差过大，所以本文选取周围 r×r 个像素的区域做块匹配。块匹配算法首先沿着**核线**前进，找到与当前块最接近的块，其间的像素距离即为所求视差。

块匹配算法的主要问题就是如何衡量两个像素块之间的差异，这也就是如何定义差异的代价函数，常用的匹配代价函数(英文：Match Cost Function)有如下几种：

1. SSD（平方差之和，Sum of Squared Differences）：
   $$
   C(i,d) = \sum_{j\in I_p} (I_p(j)-I_p'(j))^2
   $$
   
2. **SAD**（差绝对值之和，Sum of Absolute Differences）
   $$
   C(i,d) = \sum_{j\in I_p} \left| I_p(j)-I_p'(j) \right|
   $$
   
3. **ZNCC**（零均值归一化互相关，Zero Mean Normalized Cross Correlation)
   $$
   C(i,d) = \sum_{j\in I_p} \frac{(I_p(j)-\overline{I}_p)\cdot (I_p'(j)-\overline{I}_p')}{\sigma(I_p)\cdot \sigma(I_p')}
   $$
   这其中C(i,d) 表示对每个像素 i = (u, v)，视差为 d 时的代价。$I_p$ 表示右图中以 i 为中心的 r x r 的像素块，$I_p'$ 表示在左图中以 $i' = (u+d, v)$ 为中心的 r x r 的像素块。

   应用匹配代价函数，最终的视差为：
   $$
   d_i = \mathop{arg\ min}_{d\in[1,d'max]}C(i,d)
   $$
   本文实验中尝试了如上几种方法，最终选用ZNCC函数，其准确度较其他两个函数有明显优势，其他经典的匹配方法可以参考Brown发表的调研【37】。

### 4.5.2 子像素精度优化

子像素精度优化是视差估计的一个后处理过程。如果直接应用视差估计，对每个像素计算匹配代价，最终得到的视差会是一个整数，导致最终的视差图不够连续。视差的单位是像素，范围相对大一些，而后下一步生成深度图的单位是世界坐标单位，很可能值很小，这就使的深度图的断层更加明显。子像素精度优化就是求得浮点数值的视差，以避免这种情况。

如图4．5(c)，用整数计算视差，会导致得到的视差图出现“阶梯现象(英文：staircase effect)”，(d)是应用子像素精度优化的效果。
本文中使用抛物线拟合的方法来获取子像素精度，如图4．6。即选取代价最小的整数视差值d左右各n个点C(d．n)⋯．，C(d．2)，C(d．1)，C(d)，C(d+1)，C(d+2)⋯．，C(d+n)，拟合一元二次抛物线方程，最终取代价极小值点作为最终视差。

## 4.6 生成深度图算法

将视差图转化为深度图：左右两张视差图首先通过**左右一致性检测**剔除掉无效点，剔除掉无效点后的视差图再根据**三角形定律**的公式转化为深度图。

### 4.6.1 左右一致性检测

由于纹理不足、光线变化、遮挡关系等原因，会导致视差图中间的一些像素出现异常值，这种异常值如果重建到模型上，就会形成异常点，而左右一致性检测就是为了剔除这些异常点的一种处理。

### 4.6.2 三角形定律

三角形定律是利用相似三角形的原理，将视差转化为深度的公式。原理如下图:



与图4．3相似，将参考帧图二中的M2的位置在当前帧图一中标注，形成了两个相似的三角形，由相似三角形的性质可知：
$$
\frac{\delta}{f} = \frac{B}{z}
$$
这其中焦距 f 指的是 x 方向的像素焦距 fx，即视差 $\delta$ 和焦距 f 的单位都是像素，基线B和深度z的单位则是世界坐标系的单位(一般为米)。在基于对极几何的2D-2D相机位姿初始化中，会出现单目尺度不确定性，导致相机位姿的单位不确定，但是深度的单位只要与世界坐标系的单位保持一致即可。
得到深度Z：
$$
z = \frac{B\cdot f}{\delta}
$$

## 4.7 实现细节

### 4.7.1 应用图像矫正的实现

### 4.7.2 视差估计的实现

### 4.7.3 生成深度图的实现

## 5.1 模型更新模块工作流程

将**已知相机位姿的彩色图像**以及**深度图像融合**到**重建中的模型**上，以更新当前模型。本文设计了一种并行性较高的模型更新算法

![image-20201103151100373](C:\Users\MY\Documents\GitHub\cloudimg\模型更新流程图)

如图所示，模型更新模块输入为当前帧（矫正过的）相机位姿$T_i$、对应的彩色图像$I_i$、对应的深度图像$D_i$，以及截至上一帧为止的当前重建中的模型$M_{j-1}$，输出为本次更新后的模型$M_i$.

## 5.2 模型融合算法

对模型的更新，实际上就是对模型中的每个体素的值的更新，以将深度和颜色融合到模型上。算法如下：

如图5．2，对体模型中的每个体素，**假设**其在相机位姿 Ti 下的坐标为**p(X, Y, Z)**，根据投影公式得到其投影坐标 **m(x, y)**，并在彩色图像和深度图像中获取投影坐标的像素值，得到颜色值 $I_i(m)$ 和深度值$D_i(m)$。此时，如果$D_i(m) = Z$ 则说明体素p 在模型表面上。如果体素 p 在模型表面或在模型表面附近，则更新 p 位置的体素值，如果体素 p 距离模型表面太远(距离超过 $\mu$ )则选则跳过体素 p , 不对其进行更新。

### 5.2.1 体素投影的坐标变换

上述算法在体素P投影的时候实际上是存在一系列坐标变换的，本小节将依次介绍。
最初访问的体素P坐标$(X_o，Y_o，Z_o)$ 的单位是体素(voxel)，也就是体素P在 K x K x K 的模型中的索引位置，而最终的坐标(X，Y，Z)是在相机位姿 Ti 下三维坐标，单位为米(与世界坐标系的单位一致)。这其中要经过以下几个坐标变换：

1. 体素坐标系 $（X_o, Y_o, Z_o)$ 到模型坐标系 $(X_M,Y_M,Z_M)$
2. 模型坐标系 $(X_M, Y_M, Z_M)$ 到世界坐标系 $(X_W, Y_W, Z_W)$
3. 世界坐标系 $(X_W, Y_W, Z_W)$ 到照相机坐标系 $(X, Y, Z)$
4. 照相机坐标系 $(X, Y, Z)$ 到像平面 $(x,y)$

### 5.2.2 模型融合公式

## 6.1 模型投影模块工作流程

本文在TSDF模型的基础上，提出了**分层投影**的方法，本章将介绍其在整体工作流程中的应用以及具体的实现。

模型投影模块解决的问题是：仿照针孔相机模型拍照过程，**给定一个相机位姿，将重建中的模型投射到此相机位姿的像平面上，得到一张彩色图像，一张深度图像和一张权重图像。**下文中将首先介绍模型投影的光线追踪算法，然后介绍分层投影的方法。如下图6．1是模型投影模块的流程图。

## 6.2 光线追踪算法

光线追踪(英文：Ray Tracing)是一种将三维模型投影N-维屏幕的算法，
由Appel于1968年提出【4l】。如图6．2所示，模拟针孔相机模型，现实中的物体被光源照射后会进行多次反射、折射，最终射入照相机光心位置，投影在照相机的像平面。光线追踪算法一般将这个过程逆向执行，**从光心位置出发，沿着光心和像平面某一像素点m(x，y)组成的射线前进，直到第一次接触到模型表面。**

本文中不考虑物体的材质属性、光的反射、折射和阴影等，最终第一次接触到的模型表面过零点的体素值即成为像平面的像素值。

## 6.3 光线追踪算法的优化

普通的光线追踪算法从像素点出发，设定方向向量和步长，每次沿着方向向
量前进步长的长度，再检测是否与模型碰撞。这种方法复杂度高且并行性较差，本文中使用的是**体模型，所以可以对每一层的体素依次投影，最终确定像素点的值，这样对于每一层的体素，可以并行执行，提升了算法的效率。**

### 6.3.1 分层投影

### 6.3.2 按相机位姿分层

## 6.5 模型可视化软件



# [SLAM 和 SfM 的区别与联系](https://blog.csdn.net/qq_41839222/article/details/86766750#:~:text=SFM%E3%80%81%E5%8D%95%E7%9B%AE%E5%A4%9A%E8%A7%86%E8%A7%92%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E5%92%8CvSLAM%E7%9A%84%E5%8C%BA%E5%88%AB&text=SLAM%E6%98%AF%E6%9C%89%E5%BA%8F%E7%9A%84,%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%83%8F%E9%97%B4%E7%9A%84%E5%8C%B9%E9%85%8D%E3%80%82&text=3%E3%80%81SFM%20%E6%98%AF%E7%A6%BB%E7%BA%BF%E5%A4%84%E7%90%86,%E5%B5%8C%E5%85%A5%E5%BC%8F%E8%AE%BE%E5%A4%87%E4%B8%AD%E5%BA%94%E7%94%A8%E3%80%82) 

1. SFM以及单目多视角三维重建的图像是无序的，匹配时需要进行较为暴力的匹配，选择初始两张图像以及加入最好的下一张图像需要技巧；
   SLAM是有序的图像序列，一般通过前后帧间匹配和局部窗口法，只有回环检测时需要进行图像间的匹配。
2. SFM中的structure对应于vSLAM中的mapping，camera pose对应于vSLAM中location。从目的考虑，SFM主要是要为了实现三维重建（3D reconstuction），而vSLAM主要是要实现定位(location）。
3. SFM 是离线处理的，对处理时间要求不严格，但对结果的精确度要求高；SLAM需要在线处理，一般要求达到实时性。且SLAM更希望能在小型电脑或嵌入式设备中应用。

## 视觉SLAM的主要挑战

1. 回路序列和多视频序列
2. 高效高精度处理大尺度场景
3. 动态场景
4. 快速运动和强旋转问题





# ARKit

《ARKit开发实战》isbn：9787121334702 

不管是AR 还是VR，本质上都是现实世界和虚拟世界的交互程度。如果向用户展示的都是虚拟世界的界面，那就是VR；如果向用户同时显示现实世界和虚拟世界的界面，但是现实世界的成分更多，那就是AR。而不管谁更主要，都属于MR的范畴。

ARKit 包含四大模块:

1. 世界跟踪

   当相机捕捉到实时画面后，在画面中添加一个几何模型，此时手机开始运动。运动对手机的位置和姿态产生的变化都会对这个几何模型产生影响。如果距离近了，几何模型就会变大；如果围绕着几何模型移动，就可以看到几何模型不同角度的样子。所以实时获取手机位置和姿态的变化是拥有良好AR 体验的基本保障。

   ARKit 使用了一种叫做视觉惯性里程计（Visual Inertial Odometry）的技术来实时跟踪设备的位置和姿态变化。这种技术以设备初始位置为原点，然后将相机画面、惯性传感器（IMU）数据进行融合，通过对这两种数据的实时处理，就可以在不需要对外部环境进行设置、不需要预先知道外部环境状态、不需要给手机增加额外传感器的情况下，精确跟踪设备在所处环境中的运动变化。因为是对手机在现实世界里的运动进行跟踪，所以此模块被称为世界跟踪。

2. 场景理解

3. 几何渲染

4. 人脸跟踪

